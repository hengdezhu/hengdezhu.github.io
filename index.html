<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <title>Hengde Zhu</title>
  
    <meta name="author" content="Hengde Zhu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/66cbe15661.js" crossorigin="anonymous"></script>
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px"><td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center"><name>Hengde Zhu</name></p>
                <p>I am a Ph.D. student at School of Computing and Mathematical Sciences, <a href="https://le.ac.uk/">University of Leicester (UoL)</a>, United Kingdom, under the supervision of <a href="https://www.cst.cam.ac.uk/people/ss2796">Asst. Prof. Siyang Song</a>, <a href="https://le.ac.uk/people/daniel-hao">Asst. Prof. Daniel Hao</a> and <a href="https://le.ac.uk/people/rajeev-raman">Prof. Rajeev Raman</a>. I am funded by the Graduate Teaching Assistantship (GTA), University of Leicester. My research interests center on affective computing and medical image analysis. Prior to that, I earned my Master's degree in Human Technology Interaction from UoL in 2020. I completed my Bachelor's degree in Electronic Information Science & Technology from <a href="https://scnu.edu.cn/">South China Normal University (SCNU)</a> in 2017.</p>
                <p style="text-align:center">
                    <a href="mailto:hz204@le.ac.uk"><i class="fa-solid fa-envelope ai-3x" style="margin-right: 10px"></i></a>
                    <a href="https://scholar.google.co.uk/citations?user=C7yETmoAAAAJ&hl=en"><i class="ai ai-google-scholar ai-3x" style="margin-right: 10px"></i></a>
                    <a href="https://github.com/hengdezhu"><i class="fa-brands fa-github ai-3x" style="margin-right: 10px"></i></a>
                    <a href="https://www.linkedin.com/in/hengde-zhu-661ab015b"><i class="fa-brands fa-linkedin ai-3x" style="margin-right: 10px"></i></a>
                    <a href="https://orcid.org/my-orcid?orcid=0000-0001-7027-3969"><i class="fa-brands fa-orcid ai-3x" style="margin-right: 10px"></i></a>
                    <a href="https://www.instagram.com/hengdezhu"><i class="fa-brands fa-instagram ai-3x"></i></a>
                </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink">
            </td>
        </tr>
        </tbody></table>
    </td></tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:20px 5px;width:100%;vertical-align:middle;">
                    <h2 style="margin: 0; text-align: left;">News</h2>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:5px;width:15%;vertical-align:middle;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:middle;">
                            Dec, 2024
                        </div>
                    </div>
                </td>
    
                <td style="padding:5px;width:85%;vertical-align:middle;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:middle;">
                            One paper on <strong>multiple appropriate facial reaction generation</strong> has been accepted by AAAI 2025.
                        </div>
                    </div>
                </td>
            </tr>
            <tr>
                <td style="padding:5px;width:15%;vertical-align:middle;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:middle;">
                            July, 2024
                        </div>
                    </div>
                </td>
    
                <td style="padding:5px;width:85%;vertical-align:middle;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:middle;">
                            <a href="https://openreview.net/forum?id=KQVjmulG2I">One paper</a> on <strong>multiple appropriate facial reaction generation</strong> has been accepted by MM 2024.
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:20px 5px;width:100%;vertical-align:middle;">
                    <h2 style="margin: 0; text-align: left;">Selected Publications</h2>
                    <a href="https://scholar.google.co.uk/citations?user=C7yETmoAAAAJ&hl=en"><i class="fa fa-graduation-cap"></i>&nbsp Google Scholar for all publications </a>
                </td>
            </tr>
        </tbody>
    </table>
    
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:10px;width:30%;vertical-align:top;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <img src='PerFRDiff/pipeline.png' style="max-width: 100%; height: auto; display: block;">
                        </div>
                    </div>
                </td>
    
                <td style="padding:10px;width:70%;vertical-align:top;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <strong>PerFRDiff: Personalised Weight Editing for Multiple Appropriate Facial Reaction Generation</strong>
                            <br>
                            <strong>Hengde Zhu</strong>*, 
                            <a href="https://scholar.google.co.uk/citations?user=XOlqydkAAAAJ&hl=en">Xiangyu Kong</a>*,
                            <a href="https://wcxie.github.io/Weicheng-Xie">Weicheng Xie</a>,
                            <a href="http://eie.scnu.edu.cn/a/20240528/262.html">Xin Huang</a>,
                            <a href="https://csse.szu.edu.cn/pages/user/index?id=594">Linlin Shen</a>,
                            <a href="https://computerscience.exeter.ac.uk/people/profile/index.php?web_id=ll726">Lu Liu</a>, 
                            <a href="https://www.cl.cam.ac.uk/~hg410/">Hatice Gunes</a>,
                            <a href="https://www.cst.cam.ac.uk/people/ss2796">Siyang Song</a>
                            <br>
                            <em>ACM Multimedia</em>, 2024
                            <br>
                            <a href="PerFRDiff/ftp1776-zhu-3680752.pdf">[PDF]</a>
                            |
                            <a href="https://github.com/xk0720/PerFRDiff">[Code]</a>
                            |
                            <a href="PerFRDiff/supplementary_material.pdf">[Supplementary Material]</a>
                            <p>We propose the first online approach for personalized multiple appropriate facial reaction generation (MAFRG) that learns a unique cognitive style from a listener's past facial behaviors and represents it as network weight shifts. These shifts modify a pre-trained MAFRG model, enabling it to mimic the listener's cognitive process in generating appropriate facial reactions (AFRs).</p>
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:10px;width:30%;vertical-align:top;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <img src='MEEDNets/pipeline.png' style="max-width: 100%; height: auto; display: block;">
                        </div>
                    </div>
                </td>
    
                <td style="padding:10px;width:70%;vertical-align:top;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <strong>MEEDNets: Medical image classification via ensemble bio-inspired evolutionary DenseNets</strong>
                            <br>
                            <strong>Hengde Zhu</strong>,
                            <a href="https://scholar.google.co.uk/citations?user=BqlKMJ8AAAAJ&hl=en&oi=ao">Wei Wang</a>, 
                            <a href="https://www.cs.le.ac.uk/people/iulidowski">Irek Ulidowski</a>,
                            <a href="https://mathematical-ai.org/people/qinghua-zhou">Qinghua Zhou</a>,
                            <a href="https://scholar.xjtlu.edu.cn/en/persons/ShuihuaWang">Shuihua Wang</a>,
                            Huafeng Chen, 
                            <a href="https://cs.seu.edu.cn/yudongzhang/main.htm">Yudong Zhang</a>
                            <br>
                            <em>Knowledge-Based Systems</em>, 2023
                            <br>
                            <a href="https://www.sciencedirect.com/science/article/pii/S0950705123007852">[PDF]</a>
                            |
                            <a href="https://github.com/hengdezhu/MEEDNets">[Code]</a>
                            <p>This paper proposes an evolutionary mechanism inspired by biological evolution to enhance DenseNet's sparsity and efficiency for medical image classification. A synaptic model mimics asexual reproduction, passing knowledge to descendants while environmental constraints promote sparsity. Additionally, an evolution-based ensemble learning mechanism addresses the need for multiple base networks in decision-making.</p>
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:10px;width:30%;vertical-align:top;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <img src='EDCA-Net/pipeline.png' style="max-width: 100%; height: auto; display: block;">
                        </div>
                    </div>
                </td>
    
                <td style="padding:10px;width:70%;vertical-align:top;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <strong>An evolutionary attention-based network for medical image classification</strong>
                            <br>
                            <strong>Hengde Zhu</strong>,
                            <a href="https://scholar.google.co.uk/citations?user=CYGasaEAAAAJ&hl=en&oi=ao">Jian Wang</a>,
                            <a href="https://scholar.xjtlu.edu.cn/en/persons/ShuihuaWang">Shuihua Wang</a>,
                            <a href="https://le.ac.uk/people/rajeev-raman">Rajeev Raman</a>,
                            <a href="https://www.ugr.es/~gorriz/">Juan M Górriz</a>,
                            <a href="https://cs.seu.edu.cn/yudongzhang/main.htm">Yudong Zhang</a>
                            <br>
                            <em>International Journal of Neural Systems</em>, 2023
                            <br>
                            <a href="https://www.worldscientific.com/doi/abs/10.1142/S0129065723500107">[PDF]</a>
                            |
                            <a href="https://github.com/hengdezhu/EDCA-Net">[Code]</a>
                            <p>This paper introduces EDCA-Net, an evolutionary attention-based network for robust medical image classification. It builds on DCA-Net, which uses channel-wise weighted feature maps and dense connectivity for efficient information flow. Two evolutionary strategies, intra- and inter-evolution, optimize the network and enhance its generalizability by allowing weight optimization and exchange of training experiences.</p>
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <footer>
        <p>© 2024 Hengde Zhu. All Rights Reserved.</p>
    </footer>

</body>
  
</html>
