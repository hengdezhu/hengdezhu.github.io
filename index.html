<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <title>Hengde Zhu</title>
  
    <meta name="author" content="Hengde Zhu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px"><td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center"><name>Hengde Zhu</name></p>
                <p>I am a Ph.D. Student at School of Computing and Mathematical Sciences, <a href="https://le.ac.uk/">University of Leicester (UoL)</a>, United Kingdom, under the supervision of <a href="https://www.cst.cam.ac.uk/people/ss2796">Dr. Siyang Song</a> and <a href="https://le.ac.uk/people/rajeev-raman">Prof. Rajeev Raman</a>, while also being a Graduate Teaching Assistant (GTA) at the University of Leicester, funded by the Graduate Teaching Scheme. My research interests center on affective computing and medical image analysis. Previously, I earned my Master's degree in Human Technology Interaction from UoL in 2020. I completed my Bachelor's degree from <a href="https://scnu.edu.cn/">South China Normal University (SCNU)</a> in 2017.</p>
                <p style="text-align:center">
                    <a href="mailto:hz204@le.ac.uk">Email</a> &nbsp|&nbsp
                    <a href="https://scholar.google.co.uk/citations?user=C7yETmoAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
                    <a href="https://github.com/hengdezhu">Github</a> &nbsp|&nbsp
                    <a href="https://www.linkedin.com/in/hengde-zhu-661ab015b/">LinkedIn</a>
                </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink">
            </td>
        </tr>
        </tbody></table>
    </td></tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:20px 5px;width:100%;vertical-align:middle;">
                    <h2 style="margin: 0; text-align: left;">News</h2>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:5px;width:15%;vertical-align:middle;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:middle;">
                            July, 2024
                        </div>
                    </div>
                </td>
    
                <td style="padding:5px;width:85%;vertical-align:middle;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:middle;">
                            <a href="https://openreview.net/pdf?id=KQVjmulG2I">One paper</a> on <strong>multiple appropriate facial reaction generation</strong> has been accepted by MM 2024.
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:20px 5px;width:100%;vertical-align:middle;">
                    <h2 style="margin: 0; text-align: left;">Selected Publications</h2>
                </td>
            </tr>
        </tbody>
    </table>
    
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:10px;width:30%;vertical-align:top;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <img src='PerFRDiff/pipeline.png' style="max-width: 100%; height: auto; display: block;">
                        </div>
                    </div>
                </td>
    
                <td style="padding:10px;width:70%;vertical-align:top;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <strong>PerFRDiff: Personalised Weight Editing for Multiple Appropriate Facial Reaction Generation</strong>
                            <br>
                            <strong>Hengde Zhu*</strong>, Xiangyu Kong*, Weicheng Xie, Xin Huang, Linlin Shen, Lu Liu, Hatice Gunes, Siyang Song
                            <br>
                            <em>ACM Multimedia</em>, 2024
                            <br>
                            <a href="https://openreview.net/pdf?id=KQVjmulG2I">[PDF]</a>
                            |
                            <a href="https://github.com/xk0720/PerFRDiff">[Code]</a>
                            <p>We propose the first online personalised multiple appropriate facial reaction generation (MAFRG) approach which learns a unique personalised cognitive style from the target human listener's previous facial behaviours and represents it as a set of network weight shifts. These personalised weight shifts are then applied to edit the weights of a pre-trained generic MAFRG model, allowing the obtained personalised model to naturally mimic the target human listener's cognitive process in its reasoning for multiple AFRs generations.</p>
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:10px;width:30%;vertical-align:top;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <img src='MEEDNets/pipeline.png' style="max-width: 100%; height: auto; display: block;">
                        </div>
                    </div>
                </td>
    
                <td style="padding:10px;width:70%;vertical-align:top;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <strong>MEEDNets: Medical image classification via ensemble bio-inspired evolutionary DenseNets</strong>
                            <br>
                            <strong>Hengde Zhu</strong>, Wei Wang, Irek Ulidowski, Qinghua Zhou, Shuihua Wang, Huafeng Chen, Yudong Zhang
                            <br>
                            <em>Knowledge-Based Systems</em>, 2023
                            <br>
                            <a href="https://www.sciencedirect.com/science/article/pii/S0950705123007852">[PDF]</a>
                            |
                            <a href="https://github.com/hengdezhu/MEEDNets">[Code]</a>
                            <p>Inspired by the biological evolution, this paper proposes an evolutionary synthesis mechanism to automatically evolve DenseNet towards high sparsity and efficiency for medical image classification where a synaptic model is utilised to mimic biological evolution in the asexual reproduction. Each generation’s knowledge is passed down to its descendant, and an environmental constraint limits the size of the descendant evolutionary DenseNet, moving the evolution process towards high sparsity. Additionally, to address the limitation of ensemble learning that requires multiple base networks to make decisions, we propose an evolution-based ensemble learning mechanism.</p>
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
                <td style="padding:10px;width:30%;vertical-align:top;">
                    <div style="display:table;height:100%;width:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <img src='EDCA-Net/pipeline.png' style="max-width: 100%; height: auto; display: block;">
                        </div>
                    </div>
                </td>
    
                <td style="padding:10px;width:70%;vertical-align:top;">
                    <div style="display:table;height:100%;">
                        <div style="display:table-cell;vertical-align:top;">
                            <strong>An evolutionary attention-based network for medical image classification</strong>
                            <br>
                            <strong>Hengde Zhu</strong>, Jian Wang, Shui-Hua Wang, Rajeev Raman, Juan M Górriz, Yu-Dong Zhang
                            <br>
                            <em>International Journal of Neural Systems</em>, 2023
                            <br>
                            <a href="https://www.worldscientific.com/doi/abs/10.1142/S0129065723500107">[PDF]</a>
                            |
                            <a href="https://github.com/hengdezhu/EDCA-Net">[Code]</a>
                            <p>Abstract</p>
                            <p>This paper proposes an evolutionary attention-based network (EDCA-Net), which is an effective and robust network for medical image classification tasks. To extract task-related features from a given medical dataset, we first propose the densely connected attentional network (DCA-Net) where feature maps are automatically channel-wise weighted, and the dense connectivity pattern is introduced to improve the efficiency of information flow. To improve the model capability and generalizability, we introduce two types of evolution: intra- and inter-evolution. The intra-evolution optimizes the weights of DCA-Net, while the inter-evolution allows two instances of DCA-Net to exchange training experience during training. The evolutionary DCA-Net is referred to as EDCA-Net.</p>
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

</body>
  
</html>
